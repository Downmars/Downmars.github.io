---
title: "通过公网IP调用暴露的Ollama服务"
date: 2025-02-21T13:02:59+08:00
lastmod: 2025-02-21T13:02:59+08:00
draft: false

# 作者
author: "Downmars"

# 分类和标签
tags: ["deepseek", "ollama"]
categories: [""]

# 描述
description: "" # SEO 搜索优化
summary: ""    # 列表页展示的简短描述

# 自定义url
slug:

# 是否被允许搜索
searchHidden: false

# 可选：权重（用于置顶文章）
weight: null

license: "CC-BY-NC-4.0"  # 对应 data/licenses.yml 中的 keys

# 可选：封面图片
cover:
    image: ""
    alt: ""
    caption: ""
---
{{< quote >}}  
如何使用ollama-这是一个系列  
- [通过Ollama调用DeepSeek](../2025_02_09-ollama_deepseek_1)
- [通过公网IP调用暴露的Ollama服务](.):point_left: 你在这里
{{< /quote >}}


{{< admonition type="warning" >}} 使用这种方法时，请务必尊重服务器的隐私和使用政策，避免滥用公共资源。{{< /admonition >}}

随着 DeepSeek 模型的广泛应用，越来越多的人开始在自己的服务器上运行 Ollama 平台来本地执行模型。由于 Ollama 在默认配置下不需要 API 密钥，只需要知道服务器的 IP 地址和端口，就能连接并访问正在运行的模型。因此，如果你能够找到这些暴露在公网的 Ollama 实例，就能免费使用它们。

本文将介绍如何利用公网 IP 精准访问 Ollama 服务器，并使用其中的 AI 模型。

## 原理概述

Ollama 是一个支持本地运行 AI 模型的平台，在默认配置下，Ollama 不需要身份验证或 API 密钥，只要知道服务器的 IP 地址和端口，就能直接连接并访问其运行的模型。随着 DeepSeek 模型的火爆，许多服务器开始运行 Ollama 来执行这些模型。

通过简单的 GET 请求，你可以查看服务器上正在运行的模型，确认它们是否符合你的需求。这就使得你能够在没有任何授权的情况下，利用开放的服务器资源，进行“白嫖”。

## 实施步骤

### 使用 Fofa 查找 Ollama 实例

首先，你需要通过搜索引擎如 Fofa 来查找暴露了 Ollama 服务的公网 IP 地址。你可以使用以下搜索语句：
```
app="Ollama" && is_domain=false
```
这个查询帮助你定位那些运行在端口 11434 上的 Ollama 实例。这些实例通常没有做过多的访问限制，只要你知道地址，就能连接到它们。

### 导出并验证可用的服务

找到目标服务器后，你可以导出相关数据，并使用批量访问脚本进行验证。你可以自己编写脚本，也可以使用 Burp Suite 等工具进行暴力破解，检查哪些服务器开放了能够使用的接口。

### 查看正在运行的模型

一旦成功连接到服务器，你可以通过发送 GET 请求来查看服务器上运行的模型。具体地址为：
```
URL:/api/ps
```
该接口会返回服务器当前运行的所有模型的信息。如果你看到 DeepSeek 模型的名称，并且标明了参数数量（如 70B、32B），那么说明这个模型可以访问。

### 选择合适的 AI 软件进行交互

确认服务器上运行的模型后，你可以选择适合的 AI 软件来调用 Ollama 提供的 API，与模型进行交互。例如，70B 版本的模型适合处理大规模的计算任务，而 32B 版本则适合轻量级应用。

### 测试和验证模型的可用性

通过多次测试，你可以发现哪些模型可用，哪些模型由于服务器限制或负载过高可能无法访问。一般来说，70B 和 32B 是最常见的有效模型，而如 671B 等大规模模型较为罕见。

## 总结

通过这一方法，你可以在没有 API 密钥的情况下，精准地连接到正在运行 Ollama 的服务器，并使用其中的 DeepSeek 模型。只要知道目标服务器的地址和端口，你就能通过简单的 GET 请求，查看并使用这些开放的 AI 模型。这种方法被称为“精准白嫖”，它依赖于大规模使用 Ollama 的服务器的暴露服务。


